{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping to Generate a Brochure with Language Model Selection (GPT, Claude, Gemini, Ollama) and Visualization in Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\52477\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "import ollama\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure api connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai  = os.getenv(\"OPENAI_API_KEY\")\n",
    "claude  = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "gemini  = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "openai_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "gemini_model = \"gemini-1.5-flash\"\n",
    "ollama_model = \"llama3.2\"\n",
    "\n",
    "openai = OpenAI()\n",
    "claude = anthropic.Anthropic()\n",
    "gemini = google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"You are an assistant that analyzes\n",
    " the content of a company's website and creates a brief \n",
    " brochure about the company for potential clients, \n",
    " investors, and new employees. Respond in Markdown \n",
    " format. Include details about the company's culture, \n",
    " customers, careers/jobs, and courses/packages for future \n",
    " jobs if available. Add emojis.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi model response (streaming mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_openai(user_prompt):\n",
    "    # Create a streaming response from OpenAI's chat completions\n",
    "    stream = openai.chat.completions.create(\n",
    "        model    = openai_model,  # Specify the model to use\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt},  # System message to set assistant behavior\n",
    "                    {\"role\": \"system\", \"content\": user_prompt}],  # User's input message\n",
    "        stream = True  # Enable streaming mode\n",
    "    )\n",
    "    \n",
    "    ans = \"\"  # Initialize an empty string to accumulate the response\n",
    "    for chunk in stream:  # Iterate over each chunk in the streaming response\n",
    "        ans += chunk.choices[0].delta.content or \"\"  # Append the content of the chunk to the response\n",
    "        yield ans  # Yield the partial response as it is generated\n",
    "\n",
    "def stream_claude(user_prompt):\n",
    "    # Create a streaming response from Claude's model\n",
    "    stream = claude.messages.stream(\n",
    "        model  = claude_model,  # Specify the Claude model to use\n",
    "        system = system_prompt,  # Set the system message (defines assistant behavior)\n",
    "        messages = [{\"role\": \"user\", \"content\": user_prompt}],  # User's input message\n",
    "        max_tokens = 500,  # Limit the maximum number of tokens for the response\n",
    "    )\n",
    "    \n",
    "    ans = \"\"  # Initialize an empty string to accumulate the response\n",
    "    with stream as chunk:  # Iterate over each chunk of text in the streaming response\n",
    "        for text in chunk.text_stream:  # Iterate over the text stream within the chunk\n",
    "            ans += text or \"\"  # Append each piece of text to the response\n",
    "            yield ans  # Yield the partial response as it is generated\n",
    "\n",
    "def stream_gemini(user_prompt):\n",
    "    # Configure and initialize the Gemini model\n",
    "    config = google.generativeai.GenerativeModel(\n",
    "        model_name = gemini_model,  # Specify the Gemini model to use\n",
    "    )\n",
    "    \n",
    "    # Combine system prompt and user input into a single prompt\n",
    "    prompt = f\"{system_prompt}\\n{user_prompt}\"\n",
    "    \n",
    "    # Create a streaming response from the Gemini model\n",
    "    stream = config.generate_content(\n",
    "        prompt,  # Provide the combined prompt\n",
    "        stream = True  # Enable streaming mode\n",
    "    )\n",
    "    \n",
    "    ans = \"\"  # Initialize an empty string to accumulate the response\n",
    "    for chunk in stream:  # Iterate over each chunk in the streaming response\n",
    "        ans += chunk.text or \"\"  # Append the content of each chunk to the response\n",
    "        yield ans  # Yield the partial response as it is generated\n",
    "\n",
    "def stream_ollama(user_prompt):\n",
    "    # Create a streaming response from Ollama's model\n",
    "    stream = ollama.chat(\n",
    "        model = ollama_model,  # Specify the Ollama model to use\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt},  # System message to define assistant behavior\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}],  # User's input message\n",
    "        stream = True  # Enable streaming mode\n",
    "    )\n",
    "    \n",
    "    ans = \"\"  # Initialize an empty string to accumulate the response\n",
    "    for chunk in stream:  # Iterate over each chunk in the streaming response\n",
    "        ans += chunk[\"message\"][\"content\"] or \"\"  # Append the content of each chunk to the response\n",
    "        yield ans  # Yield the partial response as it is generated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webpage Content Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Webpage:\n",
    "    \"\"\"\n",
    "    A class to represent a webpage and extract its content.\n",
    "\n",
    "    Attributes:\n",
    "        url (str): The URL of the webpage.\n",
    "        title (str): The title of the webpage, or \"Sin titulo\" if not available.\n",
    "        text (str): The main body text of the webpage, excluding irrelevant elements.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Initializes the Webpage class by fetching and parsing the webpage content.\n",
    "\n",
    "        Args:\n",
    "            url (str): The URL of the webpage to be fetched and parsed.\n",
    "        \"\"\"\n",
    "        self.url = url  # Store the webpage URL\n",
    "        html = requests.get(self.url)  # Fetch the HTML content of the page\n",
    "        content = BeautifulSoup(html.content, \"html.parser\")  # Parse the HTML content using BeautifulSoup\n",
    "\n",
    "        # Extract the title of the webpage, defaulting to \"Sin titulo\" if not present\n",
    "        self.title = content.title.string if content.title else \"Sin titulo\"\n",
    "\n",
    "        # Extract the body text, removing irrelevant elements such as scripts, styles, and images\n",
    "        if content.body:\n",
    "            for irrelevant in content.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                irrelevant.decompose()  # Remove the irrelevant elements from the body\n",
    "            # Get the text from the body, joining paragraphs with newline characters and stripping extra spaces\n",
    "            self.text = content.body.get_text(separator=\"\\n\", strip=True)\n",
    "        else:\n",
    "            self.text = \"\"  # Set text to an empty string if the body is not available\n",
    "\n",
    "def get_user_prompt(nombre, webpage):\n",
    "    \"\"\"\n",
    "    Generates a user prompt by incorporating the company name and webpage content.\n",
    "\n",
    "    Args:\n",
    "        nombre (str): The name of the company.\n",
    "        webpage (str): The URL of the webpage to be processed.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string containing the company name and webpage text.\n",
    "    \"\"\"\n",
    "    webpage = Webpage(webpage)  # Create a Webpage object by passing the URL\n",
    "    return f\"\"\"\n",
    "    El nombre de la empresa es {nombre} y este es el sitio web {webpage} que contiene esta\n",
    "    informacion {webpage.text}\n",
    "    \"\"\"\n",
    "\n",
    "def get_brochure(nombre, webpage, model):\n",
    "    \"\"\"\n",
    "    Generates a brochure based on the company name, webpage content, and chosen model.\n",
    "\n",
    "    Args:\n",
    "        nombre (str): The name of the company.\n",
    "        webpage (str): The URL of the webpage to be processed.\n",
    "        model (str): The model to be used for generating the brochure. Options include \"gpt\", \"claude\", \"gemini\", and \"ollama\".\n",
    "\n",
    "    Returns:\n",
    "        generator: A generator yielding the generated response from the chosen model.\n",
    "    \"\"\"\n",
    "    user_prompt = get_user_prompt(nombre, webpage)  # Get the user prompt by processing the webpage content\n",
    "    if model == \"gpt\":\n",
    "        result = stream_openai(user_prompt)  # Use OpenAI's GPT model\n",
    "    elif model == \"claude\":\n",
    "        result = stream_claude(user_prompt)  # Use Claude model\n",
    "    elif model == \"gemini\":\n",
    "        result = stream_gemini(user_prompt)  # Use Gemini model\n",
    "    elif model == \"ollama\":\n",
    "        result = stream_ollama(user_prompt)  # Use Ollama model\n",
    "    else:\n",
    "        raise ValueError(\"Modelo Desconocido\")  # Raise an error if the model is unknown\n",
    "    yield from result  # Yield the results generated by the chosen model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\52477\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\gradio\\analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.41.0, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=get_brochure,\n",
    "    inputs=[gr.Textbox(label=\"Your company's name:\"), \n",
    "            gr.Textbox(label=\"The link with http...:\"), \n",
    "            gr.Dropdown([\"gpt\", \"claude\", \"gemini\", \"ollama\"], label=\"Select a model:\", value=\"gpt\")],\n",
    "    outputs=[gr.Markdown(label=\"Brochure:\")]\n",
    ")\n",
    "view.launch()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef5b8d18e6fdfd933dcf2d87676eea39b16d717784d6db55cec332fd8bca603e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
